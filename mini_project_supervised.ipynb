{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini poject by Ludvig Flodin - ludflo-0\n",
    "\n",
    "In this project I will try ## on five datasets (Iris, Wine, Breast cancer, Madelon, Gas sensor array drift at different concentrations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "Fix data imbalance car eval\n",
    "\n",
    "Try arrythmia using smote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Iris (150 instances, 4 features)\n",
    "\n",
    "Three different iris classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    50\n",
      "1    50\n",
      "2    50\n",
      "Name: target, dtype: int64\n",
      "(120, 4)\n",
      "(15, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "data, labels = iris.data, iris.target\n",
    "\n",
    "print(labels.value_counts())\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "iris_x_train, X_temp, iris_y_train, y_temp = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "iris_x_val, iris_x_test, iris_y_val, iris_y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "#iris_x_train, iris_x_test, iris_y_train, iris_y_test = train_test_split(data, labels, test_size=0.2, random_state=1)\n",
    "\n",
    "print(iris_x_train.shape)\n",
    "print(iris_x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 120\n",
    "iris_x_train = iris_x_train.reshape(iris_x_train.shape[0],4).astype(float)[:sample,:]\n",
    "iris_y_train = np.array(iris_y_train.astype(int)[:sample])\n",
    "\n",
    "sample = 30\n",
    "iris_x_test = iris_x_test.reshape(iris_x_test.shape[0],4).astype(float)[:sample,:]\n",
    "iris_y_test = np.array(iris_y_test.astype(int)[:sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Wine (178 instances, 13 features)\n",
    "\n",
    "The classes are three wine regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    71\n",
      "0    59\n",
      "2    48\n",
      "Name: target, dtype: int64\n",
      "(124, 13)\n",
      "(27, 13)\n",
      "(27, 13)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "wine = load_wine(as_frame = True)\n",
    "data, labels = wine.data, wine.target\n",
    "print(labels.value_counts())\n",
    "\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "wine_x_train, X_temp, wine_y_train, y_temp = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "wine_x_val, wine_x_test, wine_y_val, wine_y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "#sample = 138\n",
    "#wine_x_train = wine_x_train.reshape(wine_x_train.shape[0],13).astype(float)[:sample,:]\n",
    "#wine_y_train = np.array(wine_y_train.astype(int)[:sample])\n",
    "\n",
    "#sample = 40\n",
    "#wine_x_test = wine_x_test.reshape(wine_x_test.shape[0],13).astype(float)[:sample,:]\n",
    "#wine_y_test = np.array(wine_y_test.astype(int)[:sample])\n",
    "\n",
    "print(wine_x_train.shape)  # Should be (train_size, 13)\n",
    "print(wine_x_val.shape)    # Should be (val_size, 13)\n",
    "print(wine_x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load breast cancer (569 instances, 30 features)\n",
    "\n",
    "Binary classification i guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    357\n",
      "0    212\n",
      "Name: target, dtype: int64\n",
      "(569, 30)\n",
      "Training data shape: (455, 30)\n",
      "Validation data shape: (57, 30)\n",
      "Test data shape: (57, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cancer = load_breast_cancer(as_frame = True)\n",
    "data, labels = cancer.data, cancer.target\n",
    "print(labels.value_counts())\n",
    "print(data.shape)\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "cancer_x_train, X_temp, cancer_y_train, y_temp = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "cancer_x_val, cancer_x_test, cancer_y_val, cancer_y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {cancer_x_train.shape}\")\n",
    "print(f\"Validation data shape: {cancer_x_val.shape}\")\n",
    "print(f\"Test data shape: {cancer_x_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cancer_x_train, cancer_x_test, cancer_y_train, cancer_y_test = train_test_split(data, labels, test_size=0.2, random_state=1)\n",
    "#cancer_x_train = cancer_x_train.reshape(cancer_x_train.shape[0],30).astype(float)\n",
    "#cancer_y_train = np.array(cancer_y_train.astype(int))\n",
    "#cancer_x_test = cancer_x_test.reshape(cancer_x_test.shape[0],30).astype(float)\n",
    "#cancer_y_test = np.array(cancer_y_test.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load car eval (1728 instances, 6 features)\n",
    "\n",
    "\n",
    "evaulation level (unacceptable, acceptable, good, very good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (1728, 6)\n",
      "Labels shape: (1728,)\n",
      "Training data shape: (1382, 6)\n",
      "Validation data shape: (173, 6)\n",
      "Test data shape: (173, 6)\n",
      "Data types: car_x_train: float32, car_y_train: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\affel\\anaconda3\\envs\\nnlm\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# fetch dataset \n",
    "car_evaluation = fetch_ucirepo(id=19) \n",
    "\n",
    "data = car_evaluation.data.features\n",
    "labels = car_evaluation.data.targets\n",
    "\n",
    "data_encoded = data.copy()\n",
    "label_encoders = {} \n",
    "\n",
    "# encode str to \n",
    "for col in data.columns:\n",
    "    if data[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        data_encoded[col] = le.fit_transform(data[col])\n",
    "        label_encoders[col] = le \n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "data_encoded = np.array(data_encoded)\n",
    "labels_encoded = np.array(labels_encoded)\n",
    "\n",
    "#data = np.array(data)\n",
    "#labels = np.array(labels)\n",
    "\n",
    "car_x_train, X_temp, car_y_train, y_temp = train_test_split(data_encoded, labels_encoded, test_size=0.2, random_state=42)\n",
    "car_x_val, car_x_test, car_y_val, car_y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "car_x_train = np.array(car_x_train, dtype=np.float32)\n",
    "car_y_train = np.array(car_y_train, dtype=np.int64)\n",
    "\n",
    "car_x_val = np.array(car_x_val, dtype=np.float32)\n",
    "car_y_val = np.array(car_y_val, dtype=np.int64)\n",
    "\n",
    "car_x_test = np.array(car_x_test, dtype=np.float32)\n",
    "car_y_test = np.array(car_y_test, dtype=np.int64)\n",
    "\n",
    "print(f\"Data shape: {data_encoded.shape}\")\n",
    "print(f\"Labels shape: {labels_encoded.shape}\")\n",
    "print(f\"Training data shape: {car_x_train.shape}\")\n",
    "print(f\"Validation data shape: {car_x_val.shape}\")\n",
    "print(f\"Test data shape: {car_x_test.shape}\")\n",
    "\n",
    "print(f\"Data types: car_x_train: {car_x_train.dtype}, car_y_train: {car_y_train.dtype}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_x_train, car_x_test, car_y_train, car_y_test = train_test_split(data_encoded, labels_encoded, test_size=0.2, random_state=1)\n",
    "car_x_train = car_x_train.reshape(car_x_train.shape[0],6).astype(float)\n",
    "car_y_train = np.array(car_y_train.astype(int))\n",
    "car_x_test = car_x_test.reshape(car_x_test.shape[0],6).astype(float)\n",
    "car_y_test = np.array(car_y_test.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class SupervisedNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(SupervisedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_sizes[1], output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_supervised_model(model, train_loader, test_loader, epochs=50, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.float()\n",
    "            batch_y = batch_y.long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.float()\n",
    "            outputs = model(batch_x)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(batch_y.numpy())\n",
    "            y_pred.extend(predicted.numpy())\n",
    "\n",
    "    # Classification report and confusion matrix\n",
    "    print(\"Classification Report for Test Set\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_true), yticklabels=np.unique(y_true))\n",
    "    plt.title(\"Confusion Matrix - Supervised Model\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Data preparation function for PyTorch\n",
    "def prepare_dataloader(x_train, y_train, x_test, y_test, batch_size=32):\n",
    "    train_tensor = TensorDataset(torch.tensor(x_train).float(), torch.tensor(y_train).long())\n",
    "    test_tensor = TensorDataset(torch.tensor(x_test).float(), torch.tensor(y_test).long())\n",
    "    train_loader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_tensor, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runs for all five datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-09 12:30:18,416] A new study created in memory with name: no-name-490181d3-a455-4862-9e74-90dcfed685dc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-09 12:30:21,998] Trial 0 finished with value: 1.0 and parameters: {'BATCH_SIZE': 11, 'LEARNING_RATE': 0.0036204515127159593, 'EPOCHS': 26, 'WEIGHT_DECAY': 0.0008395588804622387}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 12:30:23,430] Trial 1 finished with value: 1.0 and parameters: {'BATCH_SIZE': 29, 'LEARNING_RATE': 0.00220247781382871, 'EPOCHS': 56, 'WEIGHT_DECAY': 3.1931680248547036e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 12:30:26,425] Trial 2 finished with value: 1.0 and parameters: {'BATCH_SIZE': 11, 'LEARNING_RATE': 0.0006178804539653005, 'EPOCHS': 47, 'WEIGHT_DECAY': 1.4567118344155244e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 12:30:26,756] Trial 3 finished with value: 0.6 and parameters: {'BATCH_SIZE': 37, 'LEARNING_RATE': 0.00012582638444234354, 'EPOCHS': 16, 'WEIGHT_DECAY': 3.4695082725373154e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 12:30:29,241] Trial 4 finished with value: 1.0 and parameters: {'BATCH_SIZE': 23, 'LEARNING_RATE': 0.0047739758255447556, 'EPOCHS': 54, 'WEIGHT_DECAY': 0.0004899936274976831}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 12:30:30,237] Trial 5 finished with value: 1.0 and parameters: {'BATCH_SIZE': 33, 'LEARNING_RATE': 0.00041364844772746607, 'EPOCHS': 53, 'WEIGHT_DECAY': 0.0002524156675199767}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 12:30:33,517] Trial 6 finished with value: 1.0 and parameters: {'BATCH_SIZE': 23, 'LEARNING_RATE': 0.00015246819791351444, 'EPOCHS': 86, 'WEIGHT_DECAY': 1.4435120352852308e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 12:30:34,074] Trial 7 finished with value: 0.8 and parameters: {'BATCH_SIZE': 23, 'LEARNING_RATE': 0.0007224747712645905, 'EPOCHS': 17, 'WEIGHT_DECAY': 0.0007274204696316272}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 12:30:44,896] Trial 8 finished with value: 0.9333333333333333 and parameters: {'BATCH_SIZE': 7, 'LEARNING_RATE': 0.0036814284368277372, 'EPOCHS': 95, 'WEIGHT_DECAY': 0.0004174023324888312}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 12:30:45,773] Trial 9 finished with value: 1.0 and parameters: {'BATCH_SIZE': 31, 'LEARNING_RATE': 0.009143525280794457, 'EPOCHS': 34, 'WEIGHT_DECAY': 1.0150061616254399e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 12:30:45,787] A new study created in memory with name: no-name-32b52beb-3444-454f-8101-d06f4b424a0a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'BATCH_SIZE': 11, 'LEARNING_RATE': 0.0036204515127159593, 'EPOCHS': 26, 'WEIGHT_DECAY': 0.0008395588804622387}\n",
      "Best accuracy: 1.0\n",
      "wine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-09 12:30:46,350] Trial 0 finished with value: 0.7407407407407407 and parameters: {'BATCH_SIZE': 45, 'LEARNING_RATE': 0.0009633977902437521, 'EPOCHS': 35, 'WEIGHT_DECAY': 0.0005894489061131226}. Best is trial 0 with value: 0.7407407407407407.\n",
      "[I 2025-01-09 12:30:46,956] Trial 1 finished with value: 0.6666666666666666 and parameters: {'BATCH_SIZE': 46, 'LEARNING_RATE': 0.00046351004983015306, 'EPOCHS': 30, 'WEIGHT_DECAY': 1.226316508496607e-05}. Best is trial 0 with value: 0.7407407407407407.\n",
      "[I 2025-01-09 12:30:49,922] Trial 2 finished with value: 0.6296296296296297 and parameters: {'BATCH_SIZE': 32, 'LEARNING_RATE': 0.002686863493161645, 'EPOCHS': 95, 'WEIGHT_DECAY': 1.9430187154111025e-05}. Best is trial 0 with value: 0.7407407407407407.\n",
      "[I 2025-01-09 12:30:52,722] Trial 3 finished with value: 0.7037037037037037 and parameters: {'BATCH_SIZE': 40, 'LEARNING_RATE': 0.0005169544335093578, 'EPOCHS': 92, 'WEIGHT_DECAY': 1.1807689079087063e-05}. Best is trial 0 with value: 0.7407407407407407.\n",
      "[I 2025-01-09 12:30:54,132] Trial 4 finished with value: 0.7407407407407407 and parameters: {'BATCH_SIZE': 42, 'LEARNING_RATE': 0.00046420338163229873, 'EPOCHS': 67, 'WEIGHT_DECAY': 1.8464664219587594e-05}. Best is trial 0 with value: 0.7407407407407407.\n",
      "[I 2025-01-09 12:30:55,842] Trial 5 finished with value: 0.7037037037037037 and parameters: {'BATCH_SIZE': 8, 'LEARNING_RATE': 0.001371849157997272, 'EPOCHS': 19, 'WEIGHT_DECAY': 1.3948574992063863e-05}. Best is trial 0 with value: 0.7407407407407407.\n",
      "[I 2025-01-09 12:30:56,770] Trial 6 finished with value: 0.37037037037037035 and parameters: {'BATCH_SIZE': 24, 'LEARNING_RATE': 0.007860483174386201, 'EPOCHS': 20, 'WEIGHT_DECAY': 5.1045621424384095e-05}. Best is trial 0 with value: 0.7407407407407407.\n",
      "[I 2025-01-09 12:30:58,141] Trial 7 finished with value: 0.7037037037037037 and parameters: {'BATCH_SIZE': 34, 'LEARNING_RATE': 0.0007295520849340438, 'EPOCHS': 63, 'WEIGHT_DECAY': 0.0009761725943886897}. Best is trial 0 with value: 0.7407407407407407.\n",
      "[I 2025-01-09 12:30:59,297] Trial 8 finished with value: 0.37037037037037035 and parameters: {'BATCH_SIZE': 26, 'LEARNING_RATE': 0.00559499623606392, 'EPOCHS': 39, 'WEIGHT_DECAY': 0.00012466658538783922}. Best is trial 0 with value: 0.7407407407407407.\n",
      "[I 2025-01-09 12:31:03,309] Trial 9 finished with value: 0.7037037037037037 and parameters: {'BATCH_SIZE': 14, 'LEARNING_RATE': 0.0004456425099088751, 'EPOCHS': 65, 'WEIGHT_DECAY': 5.770555281831494e-05}. Best is trial 0 with value: 0.7407407407407407.\n",
      "[I 2025-01-09 12:31:03,312] A new study created in memory with name: no-name-da51dea4-7912-49ed-8309-9d9badc63196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'BATCH_SIZE': 45, 'LEARNING_RATE': 0.0009633977902437521, 'EPOCHS': 35, 'WEIGHT_DECAY': 0.0005894489061131226}\n",
      "Best accuracy: 0.7407407407407407\n",
      "cancer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-09 12:31:11,976] Trial 0 finished with value: 0.2807017543859649 and parameters: {'BATCH_SIZE': 11, 'LEARNING_RATE': 0.005605442597573727, 'EPOCHS': 32, 'WEIGHT_DECAY': 0.00011151413455389613}. Best is trial 0 with value: 0.2807017543859649.\n",
      "[I 2025-01-09 12:31:29,043] Trial 1 finished with value: 0.9649122807017544 and parameters: {'BATCH_SIZE': 5, 'LEARNING_RATE': 0.0003385761699770281, 'EPOCHS': 44, 'WEIGHT_DECAY': 0.00017257190148825787}. Best is trial 1 with value: 0.9649122807017544.\n",
      "[I 2025-01-09 12:31:31,978] Trial 2 finished with value: 0.9649122807017544 and parameters: {'BATCH_SIZE': 44, 'LEARNING_RATE': 0.00011376709464825261, 'EPOCHS': 53, 'WEIGHT_DECAY': 1.315918668102213e-05}. Best is trial 1 with value: 0.9649122807017544.\n",
      "[I 2025-01-09 12:31:35,357] Trial 3 finished with value: 1.0 and parameters: {'BATCH_SIZE': 13, 'LEARNING_RATE': 0.002195986907093883, 'EPOCHS': 22, 'WEIGHT_DECAY': 0.000482918491116549}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-01-09 12:31:38,896] Trial 4 finished with value: 0.9649122807017544 and parameters: {'BATCH_SIZE': 48, 'LEARNING_RATE': 0.0008533604483555464, 'EPOCHS': 65, 'WEIGHT_DECAY': 0.00046494234576063966}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-01-09 12:31:43,304] Trial 5 finished with value: 0.7192982456140351 and parameters: {'BATCH_SIZE': 19, 'LEARNING_RATE': 0.0038108456257904856, 'EPOCHS': 46, 'WEIGHT_DECAY': 7.109345804248295e-05}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-01-09 12:31:44,081] Trial 6 finished with value: 0.9122807017543859 and parameters: {'BATCH_SIZE': 31, 'LEARNING_RATE': 0.00012162346050594425, 'EPOCHS': 12, 'WEIGHT_DECAY': 3.509345263298537e-05}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-01-09 12:31:44,665] Trial 7 finished with value: 0.9473684210526315 and parameters: {'BATCH_SIZE': 47, 'LEARNING_RATE': 0.00020445388747326813, 'EPOCHS': 12, 'WEIGHT_DECAY': 2.9074780699282395e-05}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-01-09 12:31:46,532] Trial 8 finished with value: 0.9649122807017544 and parameters: {'BATCH_SIZE': 31, 'LEARNING_RATE': 0.00020739859785222535, 'EPOCHS': 30, 'WEIGHT_DECAY': 3.2576694726004915e-05}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-01-09 12:31:54,932] Trial 9 finished with value: 0.9824561403508771 and parameters: {'BATCH_SIZE': 19, 'LEARNING_RATE': 0.0017409814155853736, 'EPOCHS': 83, 'WEIGHT_DECAY': 2.8373897973517444e-05}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-01-09 12:31:54,935] A new study created in memory with name: no-name-a5c8739b-8597-4846-9f47-b01f93ee8ef4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'BATCH_SIZE': 13, 'LEARNING_RATE': 0.002195986907093883, 'EPOCHS': 22, 'WEIGHT_DECAY': 0.000482918491116549}\n",
      "Best accuracy: 1.0\n",
      "car\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-09 12:32:23,791] Trial 0 finished with value: 0.9826589595375722 and parameters: {'BATCH_SIZE': 16, 'LEARNING_RATE': 0.0010383202617369359, 'EPOCHS': 89, 'WEIGHT_DECAY': 4.8056148294031015e-05}. Best is trial 0 with value: 0.9826589595375722.\n",
      "[I 2025-01-09 12:32:48,735] Trial 1 finished with value: 0.8265895953757225 and parameters: {'BATCH_SIZE': 17, 'LEARNING_RATE': 0.00017209511724882416, 'EPOCHS': 87, 'WEIGHT_DECAY': 0.0008080890559730607}. Best is trial 0 with value: 0.9826589595375722.\n",
      "[I 2025-01-09 12:32:52,198] Trial 2 finished with value: 0.6994219653179191 and parameters: {'BATCH_SIZE': 20, 'LEARNING_RATE': 0.00035112078113077276, 'EPOCHS': 15, 'WEIGHT_DECAY': 2.6736325380798585e-05}. Best is trial 0 with value: 0.9826589595375722.\n",
      "[I 2025-01-09 12:33:10,990] Trial 3 finished with value: 0.976878612716763 and parameters: {'BATCH_SIZE': 14, 'LEARNING_RATE': 0.001080975206416916, 'EPOCHS': 57, 'WEIGHT_DECAY': 1.8782858407330197e-05}. Best is trial 0 with value: 0.9826589595375722.\n",
      "[I 2025-01-09 12:33:16,127] Trial 4 finished with value: 0.9710982658959537 and parameters: {'BATCH_SIZE': 37, 'LEARNING_RATE': 0.0033299182592679572, 'EPOCHS': 28, 'WEIGHT_DECAY': 0.00047446222676109497}. Best is trial 0 with value: 0.9826589595375722.\n",
      "[I 2025-01-09 12:33:24,723] Trial 5 finished with value: 0.9653179190751445 and parameters: {'BATCH_SIZE': 47, 'LEARNING_RATE': 0.0011028022878314693, 'EPOCHS': 66, 'WEIGHT_DECAY': 0.00097566052864062}. Best is trial 0 with value: 0.9826589595375722.\n",
      "[I 2025-01-09 12:34:06,416] Trial 6 finished with value: 0.9075144508670521 and parameters: {'BATCH_SIZE': 11, 'LEARNING_RATE': 0.00012316051109729088, 'EPOCHS': 93, 'WEIGHT_DECAY': 8.723352098680106e-05}. Best is trial 0 with value: 0.9826589595375722.\n",
      "[I 2025-01-09 12:34:20,536] Trial 7 finished with value: 0.9710982658959537 and parameters: {'BATCH_SIZE': 24, 'LEARNING_RATE': 0.0010224883903176842, 'EPOCHS': 67, 'WEIGHT_DECAY': 2.7120943829346795e-05}. Best is trial 0 with value: 0.9826589595375722.\n",
      "[I 2025-01-09 12:34:26,310] Trial 8 finished with value: 0.9826589595375722 and parameters: {'BATCH_SIZE': 20, 'LEARNING_RATE': 0.0030222256184632918, 'EPOCHS': 24, 'WEIGHT_DECAY': 1.2714893433970632e-05}. Best is trial 0 with value: 0.9826589595375722.\n",
      "[I 2025-01-09 12:34:45,154] Trial 9 finished with value: 0.9653179190751445 and parameters: {'BATCH_SIZE': 14, 'LEARNING_RATE': 0.0018170506792544096, 'EPOCHS': 55, 'WEIGHT_DECAY': 0.00024394666930905947}. Best is trial 0 with value: 0.9826589595375722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'BATCH_SIZE': 16, 'LEARNING_RATE': 0.0010383202617369359, 'EPOCHS': 89, 'WEIGHT_DECAY': 4.8056148294031015e-05}\n",
      "Best accuracy: 0.9826589595375722\n",
      "arrhythmia\n",
      "Arrhythmia with SMOTE\n",
      "Arrhythmia with Undersampling\n",
      "Arrhythmia with SMOTEENN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Common parameters for all datasets\n",
    "BATCH_SIZE = 20\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 70\n",
    "\n",
    "def train_model(model, loss_function, optimizer, train_loader, val_loader, epochs):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model\n",
    "\n",
    "def objective(trial, x_train, y_train, x_val, y_val, input_size, num_classes):\n",
    "    # Define the model dynamically based on input and output size\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_size, trial 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 100),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(100, num_classes),\n",
    "    )\n",
    "\n",
    "    # Hyperparameter tuning with Optuna\n",
    "    BATCH_SIZE = trial.suggest_int(\"BATCH_SIZE\", 5, 50)\n",
    "    LEARNING_RATE = trial.suggest_float(\"LEARNING_RATE\", 1e-4, 1e-2, log=True)\n",
    "    EPOCHS = trial.suggest_int(\"EPOCHS\", 10, 100)\n",
    "    WEIGHT_DECAY = trial.suggest_float(\"WEIGHT_DECAY\", 1e-5, 1e-3, log=True)\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_loader = DataLoader(TensorDataset(torch.FloatTensor(x_train), torch.LongTensor(y_train)), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(torch.FloatTensor(x_val), torch.LongTensor(y_val)), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(torch.FloatTensor(x_test), torch.LongTensor(y_test)), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Train the model\n",
    "    trained_model = train_model(model, loss_function, optimizer, train_loader, val_loader, EPOCHS)\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    trained_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = trained_model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "def run_optuna(x_train, y_train, x_val, y_val, x_test, y_test, input_size, num_classes):\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, x_train, y_train, x_val, y_val, x_test, y_test, input_size, num_classes), n_trials=10)\n",
    "\n",
    "    print(\"Best parameters:\", study.best_params)\n",
    "    print(\"Best accuracy:\", study.best_value)\n",
    "\n",
    "#Iris Dataset\n",
    "print(\"iris\")\n",
    "run_optuna(\n",
    "    iris_x_train, iris_y_train, \n",
    "    iris_x_val, iris_y_val, \n",
    "    iris_x_test, iris_y_test, \n",
    "    input_size=4,  # Number of features\n",
    "    num_classes=3  # Number of classes\n",
    ")\n",
    "\n",
    "print(\"wine\")\n",
    "#wine dataset\n",
    "run_optuna(wine_x_train, wine_y_train,\n",
    "           wine_x_val, wine_y_val,\n",
    "           wine_x_test, wine_y_test,\n",
    "           input_size=13,\n",
    "           num_classes=3)\n",
    "\n",
    "\n",
    "print(\"cancer\")\n",
    "#breast cancer dataset\n",
    "run_optuna(cancer_x_train, cancer_y_train,\n",
    "           cancer_x_val, cancer_y_val,\n",
    "           cancer_x_test, cancer_y_test,\n",
    "           input_size=30,\n",
    "           num_classes=2)\n",
    "\n",
    "print(\"car\")\n",
    "#car dataset\n",
    "run_optuna(car_x_train, car_y_train,\n",
    "           car_x_val, car_y_val,\n",
    "           car_x_test, car_y_test,\n",
    "           input_size=6,\n",
    "           num_classes=4)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
