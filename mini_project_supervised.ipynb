{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini poject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Iris (150 instances, 4 features)\n",
    "\n",
    "Three different iris classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    50\n",
      "1    50\n",
      "2    50\n",
      "Name: target, dtype: int64\n",
      "(120, 4)\n",
      "(15, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "data, labels = iris.data, iris.target\n",
    "\n",
    "print(labels.value_counts())\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "iris_x_train, X_temp, iris_y_train, y_temp = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "iris_x_val, iris_x_test, iris_y_val, iris_y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "#iris_x_train, iris_x_test, iris_y_train, iris_y_test = train_test_split(data, labels, test_size=0.2, random_state=1)\n",
    "\n",
    "print(iris_x_train.shape)\n",
    "print(iris_x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 120\n",
    "iris_x_train = iris_x_train.reshape(iris_x_train.shape[0],4).astype(float)[:sample,:]\n",
    "iris_y_train = np.array(iris_y_train.astype(int)[:sample])\n",
    "\n",
    "sample = 30\n",
    "iris_x_test = iris_x_test.reshape(iris_x_test.shape[0],4).astype(float)[:sample,:]\n",
    "iris_y_test = np.array(iris_y_test.astype(int)[:sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Wine (178 instances, 13 features)\n",
    "\n",
    "The classes are three wine regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    71\n",
      "0    59\n",
      "2    48\n",
      "Name: target, dtype: int64\n",
      "(124, 13)\n",
      "(27, 13)\n",
      "(27, 13)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "wine = load_wine(as_frame = True)\n",
    "data, labels = wine.data, wine.target\n",
    "print(labels.value_counts())\n",
    "\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "wine_x_train, X_temp, wine_y_train, y_temp = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "wine_x_val, wine_x_test, wine_y_val, wine_y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "#sample = 138\n",
    "#wine_x_train = wine_x_train.reshape(wine_x_train.shape[0],13).astype(float)[:sample,:]\n",
    "#wine_y_train = np.array(wine_y_train.astype(int)[:sample])\n",
    "\n",
    "#sample = 40\n",
    "#wine_x_test = wine_x_test.reshape(wine_x_test.shape[0],13).astype(float)[:sample,:]\n",
    "#wine_y_test = np.array(wine_y_test.astype(int)[:sample])\n",
    "\n",
    "print(wine_x_train.shape)  # Should be (train_size, 13)\n",
    "print(wine_x_val.shape)    # Should be (val_size, 13)\n",
    "print(wine_x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load breast cancer (569 instances, 30 features)\n",
    "\n",
    "Binary classification i guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    357\n",
      "0    212\n",
      "Name: target, dtype: int64\n",
      "(569, 30)\n",
      "Training data shape: (455, 30)\n",
      "Validation data shape: (57, 30)\n",
      "Test data shape: (57, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cancer = load_breast_cancer(as_frame = True)\n",
    "data, labels = cancer.data, cancer.target\n",
    "print(labels.value_counts())\n",
    "print(data.shape)\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "cancer_x_train, X_temp, cancer_y_train, y_temp = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "cancer_x_val, cancer_x_test, cancer_y_val, cancer_y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {cancer_x_train.shape}\")\n",
    "print(f\"Validation data shape: {cancer_x_val.shape}\")\n",
    "print(f\"Test data shape: {cancer_x_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load car eval (1728 instances, 6 features)\n",
    "\n",
    "\n",
    "evaulation level (unacceptable, acceptable, good, very good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (1728, 6)\n",
      "Labels shape: (1728,)\n",
      "Training data shape: (1382, 6)\n",
      "Validation data shape: (173, 6)\n",
      "Test data shape: (173, 6)\n",
      "Data types: car_x_train: float32, car_y_train: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\affel\\anaconda3\\envs\\nnlm\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# fetch dataset \n",
    "car_evaluation = fetch_ucirepo(id=19) \n",
    "\n",
    "data = car_evaluation.data.features\n",
    "labels = car_evaluation.data.targets\n",
    "\n",
    "data_encoded = data.copy()\n",
    "label_encoders = {} \n",
    "\n",
    "# encode str to \n",
    "for col in data.columns:\n",
    "    if data[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        data_encoded[col] = le.fit_transform(data[col])\n",
    "        label_encoders[col] = le \n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "data_encoded = np.array(data_encoded)\n",
    "labels_encoded = np.array(labels_encoded)\n",
    "\n",
    "#data = np.array(data)\n",
    "#labels = np.array(labels)\n",
    "\n",
    "car_x_train, X_temp, car_y_train, y_temp = train_test_split(data_encoded, labels_encoded, test_size=0.2, random_state=42)\n",
    "car_x_val, car_x_test, car_y_val, car_y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "car_x_train = np.array(car_x_train, dtype=np.float32)\n",
    "car_y_train = np.array(car_y_train, dtype=np.int64)\n",
    "\n",
    "car_x_val = np.array(car_x_val, dtype=np.float32)\n",
    "car_y_val = np.array(car_y_val, dtype=np.int64)\n",
    "\n",
    "car_x_test = np.array(car_x_test, dtype=np.float32)\n",
    "car_y_test = np.array(car_y_test, dtype=np.int64)\n",
    "\n",
    "print(f\"Data shape: {data_encoded.shape}\")\n",
    "print(f\"Labels shape: {labels_encoded.shape}\")\n",
    "print(f\"Training data shape: {car_x_train.shape}\")\n",
    "print(f\"Validation data shape: {car_x_val.shape}\")\n",
    "print(f\"Test data shape: {car_x_test.shape}\")\n",
    "\n",
    "print(f\"Data types: car_x_train: {car_x_train.dtype}, car_y_train: {car_y_train.dtype}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load connectionist bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\affel\\anaconda3\\envs\\nnlm\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# fetch dataset \n",
    "connectionist_bench_sonar_mines_vs_rocks = fetch_ucirepo(id=151) \n",
    "\n",
    "data = np.array(connectionist_bench_sonar_mines_vs_rocks.data.features)\n",
    "labels = np.array(connectionist_bench_sonar_mines_vs_rocks.data.targets)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "sonar_x_train, X_temp, sonar_y_train, y_temp = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "sonar_x_val, sonar_x_test, sonar_y_val, sonar_y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class SupervisedNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(SupervisedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_sizes[1], output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_supervised_model(model, train_loader, test_loader, epochs=50, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.float()\n",
    "            batch_y = batch_y.long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.float()\n",
    "            outputs = model(batch_x)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(batch_y.numpy())\n",
    "            y_pred.extend(predicted.numpy())\n",
    "\n",
    "    # Classification report and confusion matrix\n",
    "    print(\"Classification Report for Test Set\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_true), yticklabels=np.unique(y_true))\n",
    "    plt.title(\"Confusion Matrix - Supervised Model\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Data preparation function for PyTorch\n",
    "def prepare_dataloader(x_train, y_train, x_test, y_test, batch_size=32):\n",
    "    train_tensor = TensorDataset(torch.tensor(x_train).float(), torch.tensor(y_train).long())\n",
    "    test_tensor = TensorDataset(torch.tensor(x_test).float(), torch.tensor(y_test).long())\n",
    "    train_loader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_tensor, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runs for all five datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Common parameters for all datasets\n",
    "BATCH_SIZE = 20\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 70\n",
    "\n",
    "def train_model(model, loss_function, optimizer, train_loader, val_loader, epochs):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model\n",
    "\n",
    "def objective(trial, x_train, y_train, x_val, y_val, input_size, num_classes, loss_function):\n",
    "\n",
    "    assert x_train.shape[1] == input_size, f\"Expected input_size {input_size}, but got {x_train.shape[1]}\"\n",
    "    # Define the model dynamically based on input and output size\n",
    "    hidden1 = trial.suggest_int(\"hidden1\", 50, 200)\n",
    "    hidden2 = trial.suggest_int(\"hidden2\", 50, 200)\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_size, hidden1),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden1, hidden2),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden2, num_classes),\n",
    "    )\n",
    "\n",
    "    # Hyperparameter tuning with Optuna\n",
    "    BATCH_SIZE = trial.suggest_int(\"BATCH_SIZE\", 5, 50)\n",
    "    LEARNING_RATE = trial.suggest_float(\"LEARNING_RATE\", 1e-4, 1e-2, log=True)\n",
    "    EPOCHS = trial.suggest_int(\"EPOCHS\", 10, 100)\n",
    "    WEIGHT_DECAY = trial.suggest_float(\"WEIGHT_DECAY\", 1e-5, 1e-3, log=True)\n",
    "\n",
    "    #loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_loader = DataLoader(TensorDataset(torch.FloatTensor(x_train), torch.LongTensor(y_train)), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(torch.FloatTensor(x_val), torch.LongTensor(y_val)), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Train the model\n",
    "    model.to(device)\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running optuna and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optuna(x_train, y_train, x_val, y_val, x_test, y_test, input_size, num_classes, loss_function):\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, x_train, y_train, x_val, y_val, input_size, num_classes, loss_function), n_trials=10)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(\"Best parameters:\", study.best_params)\n",
    "    print(\"Best accuracy:\", study.best_value)\n",
    "\n",
    "#run test with best parameters\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_size, best_params[\"hidden1\"]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(best_params[\"hidden1\"], best_params[\"hidden2\"]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(best_params[\"hidden2\"], num_classes),\n",
    "    )\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_params[\"LEARNING_RATE\"], weight_decay=best_params[\"WEIGHT_DECAY\"])\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(torch.FloatTensor(x_train), torch.LongTensor(y_train)), batch_size=best_params[\"BATCH_SIZE\"], shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(torch.FloatTensor(x_test), torch.LongTensor(y_test)), batch_size=best_params[\"BATCH_SIZE\"], shuffle=False)\n",
    "\n",
    "    final_model = train_model(model, loss_function, optimizer, test_loader, None, best_params[\"EPOCHS\"])\n",
    "\n",
    "    final_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = final_model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = correct / total\n",
    "    print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run optuna for all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-09 20:42:29,697] A new study created in memory with name: no-name-07802f32-6e04-4f47-a0d3-63c7c8c1490a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-09 20:42:30,589] Trial 0 finished with value: 1.0 and parameters: {'hidden1': 188, 'hidden2': 80, 'BATCH_SIZE': 24, 'LEARNING_RATE': 0.00441057808113851, 'EPOCHS': 40, 'WEIGHT_DECAY': 0.00017836648217043794}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 20:42:31,003] Trial 1 finished with value: 1.0 and parameters: {'hidden1': 60, 'hidden2': 152, 'BATCH_SIZE': 30, 'LEARNING_RATE': 0.006645184440143127, 'EPOCHS': 20, 'WEIGHT_DECAY': 5.725678918497414e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 20:42:34,455] Trial 2 finished with value: 0.9333333333333333 and parameters: {'hidden1': 76, 'hidden2': 85, 'BATCH_SIZE': 9, 'LEARNING_RATE': 0.0004217310235730437, 'EPOCHS': 83, 'WEIGHT_DECAY': 0.0005298331820498568}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 20:42:35,229] Trial 3 finished with value: 1.0 and parameters: {'hidden1': 68, 'hidden2': 87, 'BATCH_SIZE': 43, 'LEARNING_RATE': 0.0014560276859808326, 'EPOCHS': 66, 'WEIGHT_DECAY': 1.2502059505830056e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 20:42:36,218] Trial 4 finished with value: 1.0 and parameters: {'hidden1': 79, 'hidden2': 56, 'BATCH_SIZE': 42, 'LEARNING_RATE': 0.00044432501724404546, 'EPOCHS': 69, 'WEIGHT_DECAY': 5.950020273886844e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 20:42:36,906] Trial 5 finished with value: 1.0 and parameters: {'hidden1': 65, 'hidden2': 74, 'BATCH_SIZE': 30, 'LEARNING_RATE': 0.0006688610803448014, 'EPOCHS': 36, 'WEIGHT_DECAY': 4.346364912682256e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 20:42:38,956] Trial 6 finished with value: 1.0 and parameters: {'hidden1': 102, 'hidden2': 186, 'BATCH_SIZE': 38, 'LEARNING_RATE': 0.003946981231681536, 'EPOCHS': 94, 'WEIGHT_DECAY': 1.3282176920712844e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 20:42:39,904] Trial 7 finished with value: 0.8666666666666667 and parameters: {'hidden1': 94, 'hidden2': 169, 'BATCH_SIZE': 47, 'LEARNING_RATE': 0.00010488668425098134, 'EPOCHS': 54, 'WEIGHT_DECAY': 3.289289001755408e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 20:42:41,630] Trial 8 finished with value: 1.0 and parameters: {'hidden1': 88, 'hidden2': 181, 'BATCH_SIZE': 42, 'LEARNING_RATE': 0.00011137573067404999, 'EPOCHS': 94, 'WEIGHT_DECAY': 4.3036912966090896e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 20:42:44,110] Trial 9 finished with value: 1.0 and parameters: {'hidden1': 158, 'hidden2': 64, 'BATCH_SIZE': 10, 'LEARNING_RATE': 0.000136424585973663, 'EPOCHS': 45, 'WEIGHT_DECAY': 2.1320044654652584e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 20:42:44,296] A new study created in memory with name: no-name-fa328eef-c645-4ef8-9139-7b28f4e1f3e2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'hidden1': 188, 'hidden2': 80, 'BATCH_SIZE': 24, 'LEARNING_RATE': 0.00441057808113851, 'EPOCHS': 40, 'WEIGHT_DECAY': 0.00017836648217043794}\n",
      "Best accuracy: 1.0\n",
      "Test accuracy: 1.0\n",
      "wine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-09 20:42:44,909] Trial 0 finished with value: 1.0 and parameters: {'hidden1': 91, 'hidden2': 51, 'BATCH_SIZE': 43, 'LEARNING_RATE': 0.007996522328357267, 'EPOCHS': 46, 'WEIGHT_DECAY': 2.3898003005528288e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 20:42:45,804] Trial 1 finished with value: 1.0 and parameters: {'hidden1': 81, 'hidden2': 121, 'BATCH_SIZE': 45, 'LEARNING_RATE': 0.0046360305082980635, 'EPOCHS': 63, 'WEIGHT_DECAY': 7.932237808397186e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 20:42:47,791] Trial 2 finished with value: 0.9629629629629629 and parameters: {'hidden1': 174, 'hidden2': 111, 'BATCH_SIZE': 36, 'LEARNING_RATE': 0.0002313718291372224, 'EPOCHS': 92, 'WEIGHT_DECAY': 2.531680505017614e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 20:42:48,854] Trial 3 finished with value: 0.8518518518518519 and parameters: {'hidden1': 97, 'hidden2': 143, 'BATCH_SIZE': 37, 'LEARNING_RATE': 0.0003307664136011357, 'EPOCHS': 62, 'WEIGHT_DECAY': 1.5075399394675509e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 20:42:50,347] Trial 4 finished with value: 0.48148148148148145 and parameters: {'hidden1': 147, 'hidden2': 192, 'BATCH_SIZE': 9, 'LEARNING_RATE': 0.0002621809125317893, 'EPOCHS': 27, 'WEIGHT_DECAY': 0.00019508715247937937}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 20:42:51,532] Trial 5 finished with value: 0.8148148148148148 and parameters: {'hidden1': 147, 'hidden2': 74, 'BATCH_SIZE': 36, 'LEARNING_RATE': 0.0038312704836608756, 'EPOCHS': 69, 'WEIGHT_DECAY': 0.0001417806255845561}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 20:42:53,131] Trial 6 finished with value: 0.8518518518518519 and parameters: {'hidden1': 168, 'hidden2': 177, 'BATCH_SIZE': 46, 'LEARNING_RATE': 0.0018060372551488508, 'EPOCHS': 100, 'WEIGHT_DECAY': 0.0005678098741373732}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 20:42:54,779] Trial 7 finished with value: 0.8888888888888888 and parameters: {'hidden1': 141, 'hidden2': 98, 'BATCH_SIZE': 34, 'LEARNING_RATE': 0.007736647335268332, 'EPOCHS': 92, 'WEIGHT_DECAY': 1.1858136547607327e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 20:42:56,093] Trial 8 finished with value: 0.7777777777777778 and parameters: {'hidden1': 187, 'hidden2': 160, 'BATCH_SIZE': 42, 'LEARNING_RATE': 0.00030076838102454403, 'EPOCHS': 72, 'WEIGHT_DECAY': 0.0006538711637944273}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 20:42:58,246] Trial 9 finished with value: 0.6666666666666666 and parameters: {'hidden1': 190, 'hidden2': 146, 'BATCH_SIZE': 11, 'LEARNING_RATE': 0.0003514684185762686, 'EPOCHS': 39, 'WEIGHT_DECAY': 8.273839885105595e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-09 20:42:58,425] A new study created in memory with name: no-name-24a4376f-ee31-4d21-bc34-f1a814f7cfb2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'hidden1': 91, 'hidden2': 51, 'BATCH_SIZE': 43, 'LEARNING_RATE': 0.007996522328357267, 'EPOCHS': 46, 'WEIGHT_DECAY': 2.3898003005528288e-05}\n",
      "Best accuracy: 1.0\n",
      "Test accuracy: 0.8148148148148148\n",
      "cancer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-09 20:43:02,971] Trial 0 finished with value: 0.9298245614035088 and parameters: {'hidden1': 146, 'hidden2': 155, 'BATCH_SIZE': 40, 'LEARNING_RATE': 0.00026918916508758117, 'EPOCHS': 76, 'WEIGHT_DECAY': 0.0006390892187244817}. Best is trial 0 with value: 0.9298245614035088.\n",
      "[I 2025-01-09 20:43:10,940] Trial 1 finished with value: 0.8947368421052632 and parameters: {'hidden1': 92, 'hidden2': 107, 'BATCH_SIZE': 10, 'LEARNING_RATE': 0.005512402919248426, 'EPOCHS': 44, 'WEIGHT_DECAY': 0.00011292910752855219}. Best is trial 0 with value: 0.9298245614035088.\n",
      "[I 2025-01-09 20:43:13,232] Trial 2 finished with value: 0.9473684210526315 and parameters: {'hidden1': 160, 'hidden2': 143, 'BATCH_SIZE': 27, 'LEARNING_RATE': 0.009159383562661142, 'EPOCHS': 24, 'WEIGHT_DECAY': 1.3651504461877988e-05}. Best is trial 2 with value: 0.9473684210526315.\n",
      "[I 2025-01-09 20:43:14,498] Trial 3 finished with value: 0.9473684210526315 and parameters: {'hidden1': 149, 'hidden2': 125, 'BATCH_SIZE': 43, 'LEARNING_RATE': 0.0015072465213655645, 'EPOCHS': 21, 'WEIGHT_DECAY': 0.00025692163885918345}. Best is trial 2 with value: 0.9473684210526315.\n",
      "[I 2025-01-09 20:43:34,735] Trial 4 finished with value: 0.9649122807017544 and parameters: {'hidden1': 145, 'hidden2': 119, 'BATCH_SIZE': 9, 'LEARNING_RATE': 0.0008229811352502577, 'EPOCHS': 83, 'WEIGHT_DECAY': 1.2631775844448772e-05}. Best is trial 4 with value: 0.9649122807017544.\n",
      "[I 2025-01-09 20:43:54,693] Trial 5 finished with value: 0.9122807017543859 and parameters: {'hidden1': 159, 'hidden2': 196, 'BATCH_SIZE': 14, 'LEARNING_RATE': 0.0010865330602756248, 'EPOCHS': 97, 'WEIGHT_DECAY': 6.529924495609696e-05}. Best is trial 4 with value: 0.9649122807017544.\n",
      "[I 2025-01-09 20:44:02,276] Trial 6 finished with value: 0.9649122807017544 and parameters: {'hidden1': 151, 'hidden2': 50, 'BATCH_SIZE': 28, 'LEARNING_RATE': 0.0008580513930387163, 'EPOCHS': 94, 'WEIGHT_DECAY': 0.0001422792652222098}. Best is trial 4 with value: 0.9649122807017544.\n",
      "[I 2025-01-09 20:44:22,050] Trial 7 finished with value: 0.9649122807017544 and parameters: {'hidden1': 156, 'hidden2': 80, 'BATCH_SIZE': 6, 'LEARNING_RATE': 0.006337209183937112, 'EPOCHS': 59, 'WEIGHT_DECAY': 2.0817830920433734e-05}. Best is trial 4 with value: 0.9649122807017544.\n",
      "[I 2025-01-09 20:44:32,403] Trial 8 finished with value: 0.8947368421052632 and parameters: {'hidden1': 169, 'hidden2': 176, 'BATCH_SIZE': 13, 'LEARNING_RATE': 0.00010603085028921259, 'EPOCHS': 56, 'WEIGHT_DECAY': 0.0006451579115935074}. Best is trial 4 with value: 0.9649122807017544.\n",
      "[I 2025-01-09 20:44:33,601] Trial 9 finished with value: 0.8947368421052632 and parameters: {'hidden1': 144, 'hidden2': 173, 'BATCH_SIZE': 50, 'LEARNING_RATE': 0.00025103123212728444, 'EPOCHS': 19, 'WEIGHT_DECAY': 0.000250072471823764}. Best is trial 4 with value: 0.9649122807017544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'hidden1': 145, 'hidden2': 119, 'BATCH_SIZE': 9, 'LEARNING_RATE': 0.0008229811352502577, 'EPOCHS': 83, 'WEIGHT_DECAY': 1.2631775844448772e-05}\n",
      "Best accuracy: 0.9649122807017544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-09 20:44:35,944] A new study created in memory with name: no-name-18ccf9e6-9209-4d5f-bde5-e550c6774fc4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 1.0\n",
      "car\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-09 20:44:46,086] Trial 0 finished with value: 0.9884393063583815 and parameters: {'hidden1': 93, 'hidden2': 199, 'BATCH_SIZE': 36, 'LEARNING_RATE': 0.004031518328563456, 'EPOCHS': 53, 'WEIGHT_DECAY': 0.00018422688370546583}. Best is trial 0 with value: 0.9884393063583815.\n",
      "[I 2025-01-09 20:44:48,891] Trial 1 finished with value: 0.6473988439306358 and parameters: {'hidden1': 78, 'hidden2': 199, 'BATCH_SIZE': 44, 'LEARNING_RATE': 0.00016922275461734446, 'EPOCHS': 14, 'WEIGHT_DECAY': 0.0007067755639524518}. Best is trial 0 with value: 0.9884393063583815.\n",
      "[I 2025-01-09 20:44:52,640] Trial 2 finished with value: 0.9826589595375722 and parameters: {'hidden1': 133, 'hidden2': 157, 'BATCH_SIZE': 44, 'LEARNING_RATE': 0.001671599877067647, 'EPOCHS': 19, 'WEIGHT_DECAY': 2.4391683013526512e-05}. Best is trial 0 with value: 0.9884393063583815.\n",
      "[I 2025-01-09 20:45:14,977] Trial 3 finished with value: 0.9710982658959537 and parameters: {'hidden1': 115, 'hidden2': 190, 'BATCH_SIZE': 19, 'LEARNING_RATE': 0.00019345842176493272, 'EPOCHS': 46, 'WEIGHT_DECAY': 3.928978421529075e-05}. Best is trial 0 with value: 0.9884393063583815.\n",
      "[I 2025-01-09 20:45:29,714] Trial 4 finished with value: 0.9942196531791907 and parameters: {'hidden1': 179, 'hidden2': 160, 'BATCH_SIZE': 18, 'LEARNING_RATE': 0.001048975415487704, 'EPOCHS': 27, 'WEIGHT_DECAY': 5.6186578894388765e-05}. Best is trial 4 with value: 0.9942196531791907.\n",
      "[I 2025-01-09 20:45:42,653] Trial 5 finished with value: 0.9653179190751445 and parameters: {'hidden1': 64, 'hidden2': 76, 'BATCH_SIZE': 42, 'LEARNING_RATE': 0.0016759149116008054, 'EPOCHS': 56, 'WEIGHT_DECAY': 0.00015060636593078455}. Best is trial 4 with value: 0.9942196531791907.\n",
      "[I 2025-01-09 20:46:05,642] Trial 6 finished with value: 0.9942196531791907 and parameters: {'hidden1': 104, 'hidden2': 63, 'BATCH_SIZE': 40, 'LEARNING_RATE': 0.0029622361773427863, 'EPOCHS': 92, 'WEIGHT_DECAY': 0.00016527256106555838}. Best is trial 4 with value: 0.9942196531791907.\n",
      "[I 2025-01-09 20:46:39,521] Trial 7 finished with value: 0.9942196531791907 and parameters: {'hidden1': 191, 'hidden2': 179, 'BATCH_SIZE': 8, 'LEARNING_RATE': 0.003009074161163042, 'EPOCHS': 31, 'WEIGHT_DECAY': 3.25752934271148e-05}. Best is trial 4 with value: 0.9942196531791907.\n",
      "[I 2025-01-09 20:46:48,933] Trial 8 finished with value: 0.8497109826589595 and parameters: {'hidden1': 144, 'hidden2': 124, 'BATCH_SIZE': 15, 'LEARNING_RATE': 0.00018139962955868787, 'EPOCHS': 26, 'WEIGHT_DECAY': 0.00014874923811263204}. Best is trial 4 with value: 0.9942196531791907.\n",
      "[I 2025-01-09 20:46:59,286] Trial 9 finished with value: 0.8959537572254336 and parameters: {'hidden1': 58, 'hidden2': 152, 'BATCH_SIZE': 39, 'LEARNING_RATE': 0.00018083505749871565, 'EPOCHS': 75, 'WEIGHT_DECAY': 4.818433736629988e-05}. Best is trial 4 with value: 0.9942196531791907.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'hidden1': 179, 'hidden2': 160, 'BATCH_SIZE': 18, 'LEARNING_RATE': 0.001048975415487704, 'EPOCHS': 27, 'WEIGHT_DECAY': 5.6186578894388765e-05}\n",
      "Best accuracy: 0.9942196531791907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-09 20:47:00,253] A new study created in memory with name: no-name-f90c638b-35b9-4fdf-8f28-9d023f4e5bce\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.930635838150289\n",
      "connectionist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-09 20:47:01,904] Trial 0 finished with value: 0.7619047619047619 and parameters: {'hidden1': 103, 'hidden2': 85, 'BATCH_SIZE': 43, 'LEARNING_RATE': 0.00022131231794676984, 'EPOCHS': 85, 'WEIGHT_DECAY': 0.0007573450314513134}. Best is trial 0 with value: 0.7619047619047619.\n",
      "[I 2025-01-09 20:47:02,504] Trial 1 finished with value: 0.7619047619047619 and parameters: {'hidden1': 137, 'hidden2': 155, 'BATCH_SIZE': 16, 'LEARNING_RATE': 0.0010172646189562085, 'EPOCHS': 12, 'WEIGHT_DECAY': 0.00032653640189825366}. Best is trial 0 with value: 0.7619047619047619.\n",
      "[I 2025-01-09 20:47:04,904] Trial 2 finished with value: 0.7619047619047619 and parameters: {'hidden1': 96, 'hidden2': 173, 'BATCH_SIZE': 37, 'LEARNING_RATE': 0.00020834477639503985, 'EPOCHS': 98, 'WEIGHT_DECAY': 0.00022667630730251423}. Best is trial 0 with value: 0.7619047619047619.\n",
      "[I 2025-01-09 20:47:06,938] Trial 3 finished with value: 0.8571428571428571 and parameters: {'hidden1': 142, 'hidden2': 170, 'BATCH_SIZE': 27, 'LEARNING_RATE': 0.0012697248237720094, 'EPOCHS': 61, 'WEIGHT_DECAY': 0.0002099643067990804}. Best is trial 3 with value: 0.8571428571428571.\n",
      "[I 2025-01-09 20:47:09,580] Trial 4 finished with value: 0.7142857142857143 and parameters: {'hidden1': 164, 'hidden2': 80, 'BATCH_SIZE': 16, 'LEARNING_RATE': 0.0004479537594801399, 'EPOCHS': 49, 'WEIGHT_DECAY': 2.1378069444696048e-05}. Best is trial 3 with value: 0.8571428571428571.\n",
      "[I 2025-01-09 20:47:11,524] Trial 5 finished with value: 0.7619047619047619 and parameters: {'hidden1': 117, 'hidden2': 97, 'BATCH_SIZE': 25, 'LEARNING_RATE': 0.0002754396573660033, 'EPOCHS': 51, 'WEIGHT_DECAY': 0.00028981603068623657}. Best is trial 3 with value: 0.8571428571428571.\n",
      "[I 2025-01-09 20:47:13,290] Trial 6 finished with value: 0.7619047619047619 and parameters: {'hidden1': 62, 'hidden2': 112, 'BATCH_SIZE': 7, 'LEARNING_RATE': 0.00027953934686312013, 'EPOCHS': 21, 'WEIGHT_DECAY': 4.91970256783801e-05}. Best is trial 3 with value: 0.8571428571428571.\n",
      "[I 2025-01-09 20:47:16,114] Trial 7 finished with value: 0.8571428571428571 and parameters: {'hidden1': 192, 'hidden2': 89, 'BATCH_SIZE': 26, 'LEARNING_RATE': 0.0010192926769217567, 'EPOCHS': 79, 'WEIGHT_DECAY': 3.408853441053872e-05}. Best is trial 3 with value: 0.8571428571428571.\n",
      "[I 2025-01-09 20:47:16,541] Trial 8 finished with value: 0.8095238095238095 and parameters: {'hidden1': 176, 'hidden2': 104, 'BATCH_SIZE': 29, 'LEARNING_RATE': 0.0034259624992415806, 'EPOCHS': 14, 'WEIGHT_DECAY': 0.0008251807086326053}. Best is trial 3 with value: 0.8571428571428571.\n",
      "[I 2025-01-09 20:47:19,342] Trial 9 finished with value: 0.8571428571428571 and parameters: {'hidden1': 150, 'hidden2': 169, 'BATCH_SIZE': 39, 'LEARNING_RATE': 0.003415917736601962, 'EPOCHS': 100, 'WEIGHT_DECAY': 3.4238058742644056e-05}. Best is trial 3 with value: 0.8571428571428571.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'hidden1': 142, 'hidden2': 170, 'BATCH_SIZE': 27, 'LEARNING_RATE': 0.0012697248237720094, 'EPOCHS': 61, 'WEIGHT_DECAY': 0.0002099643067990804}\n",
      "Best accuracy: 0.8571428571428571\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "#Iris Dataset\n",
    "print(\"iris\")\n",
    "run_optuna(\n",
    "        iris_x_train, iris_y_train, \n",
    "        iris_x_val, iris_y_val, \n",
    "        iris_x_test, iris_y_test, \n",
    "        input_size=4,  # Number of features\n",
    "        num_classes=3,  # Number of classes\n",
    "        loss_function = CrossEntropyLoss()\n",
    "    )\n",
    "\n",
    "print(\"wine\")\n",
    "#wine dataset\n",
    "run_optuna(wine_x_train, wine_y_train,\n",
    "        wine_x_val, wine_y_val,\n",
    "        wine_x_test, wine_y_test,\n",
    "        input_size=13,\n",
    "        num_classes=3,\n",
    "        loss_function = CrossEntropyLoss()\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"cancer\")\n",
    "#breast cancer dataset\n",
    "run_optuna(cancer_x_train, cancer_y_train,\n",
    "        cancer_x_val, cancer_y_val,\n",
    "        cancer_x_test, cancer_y_test,\n",
    "        input_size=30,\n",
    "        num_classes=2,\n",
    "        loss_function = CrossEntropyLoss()\n",
    "    )\n",
    "\n",
    "print(\"car\")\n",
    "#car dataset\n",
    "run_optuna(car_x_train, car_y_train,\n",
    "        car_x_val, car_y_val,\n",
    "        car_x_test, car_y_test,\n",
    "        input_size=6,\n",
    "        num_classes=4,\n",
    "        loss_function = CrossEntropyLoss()\n",
    "    )\n",
    "\n",
    "print(\"connectionist\")\n",
    "#car dataset\n",
    "run_optuna(sonar_x_train, sonar_y_train,\n",
    "        sonar_x_val, sonar_y_val,\n",
    "        sonar_x_test, sonar_y_test,\n",
    "        input_size=60,\n",
    "        num_classes=2,\n",
    "        loss_function = CrossEntropyLoss()\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
